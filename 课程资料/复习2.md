### 机器学习和数据挖掘简介

[toc]

1. 什么是机器学习？数据挖掘的定义及其类型。
    学习是系统根据过去的经验提高其在特定任务上的表现的过程，机器学习也是关于如何根据过去的经验提高特定任务的表现，但它是通过学习算法来实现的。机器挖掘是用机器学习方法从海量数据中发现有用的知识，其有两种类型：描述类型：查找隐藏在数据中的人类可解释模式数据（聚类、可解释的表示（例如，非负矩阵分解、主题模型））；预测类型：使用一些变量来预测其他变量的未知值或未来值（推荐系统、异常检测、类别预测）。
2. 传统算法与学习算法

|![image-20250108200537226](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108200537226.png)|![image-20250108200551592](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108200551592.png)|
|--|--|

3. 机器学习的类型
* 监督学习 − 给定：输入数据 + 预测
	给定输入数据和监督信号 $(𝑥_i, 𝑦_i)$ 对，学习一个函数 𝑓(𝑥) 来预测新输入 𝑥 的 𝑦 值
	
* 无监督学习 − 给定：输入数据
	给定输入数据$𝑥_1, 𝑥_2, ⋯ , 𝑥_n$（无预测），输出隐藏在数据$𝑥_i$ 中的信息

* 半无监督学习

* 弱监督学习

* 自我无监督学习

* 强化学习 − 给定：动作序列 + 奖励

### 线性回归

1. 什么是线性回归？
    根据给定的特征，预测感兴趣的变量的值。从数学上讲，回归旨在学习一个函数 𝑓，该函数可以模拟输入数据 𝑥 和输出值 𝑦 之间的关系$y = f(x_1, x_2, \dots, x_m)$​，线性回归模型将函数族 𝑓(⋅) 限制为线性形式，线性回归的最终目标是尽可能接近训练数据集中所有数据样本的真实值𝑦，应用于房价预测、销售额预测、温度变化建模。

$$
f(x_1, x_2, \dots, x_m) = w_0 + w_1x_1 + w_2x_2 + \dots + w_mx_m
$$

在线性回归中，最常用的代价函数是 **均方误差（Mean Squared Error, MSE）**，数学表达式为：

$$
L(w_0, w_1, \dots, w_m) = \frac{1}{n} \sum_{i=1}^{n} \left( f(x^{(i)}) - y^{(i)} \right)^2
$$

2. 小批量梯度下降 
小批量梯度下降的核心思想是，每次迭代不使用整个数据集计算梯度，而是随机选取一部分数据（小批量，mini-batch）来近似计算梯度，从而降低计算复杂度。

- 真实梯度公式：
$$
  \frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial \ell(\mathbf{w}, \mathbf{x}_i, y_i)}{\partial \mathbf{w}}
$$
- 小批量梯度公式：
$$
\frac{\partial L_{\text{mini-batch}}(\mathbf{w})}{\partial \mathbf{w}} = \frac{1}{|\mathcal{B}|} \sum_{j \in \mathcal{B}} \frac{\partial \ell(\mathbf{w}, \mathbf{x}_j, y_j)}{\partial \mathbf{w}}
$$
  其中：$ \mathcal{B} $ 表示小批量数据集，包含 $ |\mathcal{B}| $ 个样本。$ |\mathcal{B}| \ll n $，通常 $ |\mathcal{B}| $ 为 16、32、64 等较小的值。

* 真实梯度：基于整个数据集计算，是损失函数的精确梯度。更稳定，但计算代价高，尤其当数据量非常大时。
* 小批量梯度：是真实梯度的一个随机近似值，使用小批量数据计算。每次迭代的梯度存在随机噪声，但随着迭代次数增多，模型参数会逐渐收敛到与使用真实梯度相近的结果。

### 线性分类器

1. 逻辑回归：逻辑回归用于解决**分类问题**，尤其是二分类问题，例如垃圾邮件检测或疾病诊断。

![image-20250108205022445](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108205022445.png)

![image-20250108205414983](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108205414983.png)

逻辑回归输出一个概率值，表示样本属于某个类别的可能性。通过将线性组合的结果输入到**Sigmoid 函数**中，实现概率输出，目标变量 $y$ 是离散值，通常为 $y \in \{0, 1\}$（二分类）。
$$
p(y=1|\mathbf{x}) = \sigma(\mathbf{x}^\top \mathbf{w}) = \frac{1}{1 + e^{-\mathbf{x}^\top \mathbf{w}}}
$$
其中：
- $ \mathbf{x}^\top \mathbf{w} $ 是线性部分。
- $ \sigma(z) $ 是 Sigmoid 激活函数，输出范围为 $[0, 1]$。

![image-20250108205741548](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108205741548.png)

使用**对数损失函数 (Log Loss)**：
$$
L(w) = -\frac{1}{n} \sum_{i=1}^n \left( y^{(i)} \log(p^{(i)}) + (1 - y^{(i)}) \log(1 - p^{(i)}) \right)
$$
其中 $ p^{(i)} $ 是第 $i$ 个样本预测为正类的概率。

![image-20250108205611607](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108205611607.png)

2. 处理多分类问题的两种方法

* One-vs-All：将多分类问题转化为多个二分类问题
  对于线性多分类问题，我们最初使用的方法是将K分类问题转换为K个二分类问题，要预测一个新的样本$\mathbf{x}$的类别，最常用的方法之一是选择使得某个得分函数最大的类别。
  ![image-20241123200612827](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123200612827.png)
$$
k = \arg \max_{j} \, f_j(\mathbf{x})
$$

* Softmax 函数：直接将样本归类到某一类

  很明显上述方法效率较低，由此我们引入了softmax函数。在数学，特别是概率论和机器学习领域中，**Softmax函数**（也称归一化指数函数）是逻辑回归函数（Logistic function）的一种推广。它可以将一个任意实数的$K$维向量$\mathbf{z}$，压缩成一个$K$维的实数向量$\sigma(\mathbf{z})$，使得每个元素都在$(0,1)$之间，且所有元素的和为1。可以理解为该向量表示一个概率分布。

  Softmax函数通常定义如下：
$$
\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \quad \text{for } j = 1, \dots, K
$$
  其中：
  - $\mathbf{z} = [z_1, z_2, \dots, z_K]$是输入的$K$维向量。
  - $\sigma(\mathbf{z})_j$是Softmax输出向量$\sigma(\mathbf{z})$的第$j$个元素。

  在多项逻辑回归和线性判别分析中，Softmax函数的输入通常是从$K$个不同线性函数得到的结果。样本向量$\mathbf{x}$属于第$j$个类别的概率为：
$$
  P(y=j|\mathbf{x}) = \frac{e^{\mathbf{x}^\mathsf{T} \mathbf{w}_j}}{\sum_{k=1}^K e^{\mathbf{x}^\mathsf{T} \mathbf{w}_k}}
$$
  其中：
  - $\mathbf{x}$是输入样本向量。
  - $\mathbf{w}_j$是类别$j$对应的参数向量。
  - $K$是类别的总数。

  𝐾 = 2 的 softmax 函数等同于逻辑函数：

  ![image-20250108210733957](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108210733957.png)

  ![image-20250108211621710](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108211621710.png)

  **实际上Softmax 分类器可以看作是一个最简单的 MLP，即单层感知机。它只有一个输入层和一个输出层，没有隐藏层。换句话说，Softmax 分类器相当于一个 单层神经网络。MLP 则是在这个基础上加入了 隐藏层。因此，当将 MLP 的隐藏层数量设置为 0 时，MLP 就等价于一个 Softmax 分类器。**

3. 从概率建模的角度来看，线性回归实际上等同于：建模：假设条件分布为对角线高斯；训练：通过最大化对数似然来训练模型。逻辑回归相当于：建模：假设输出服从伯努利条件分布；训练：通过最大化对数似然来训练模型。回归、逻辑回归和多类回归可以放在一个共同的框架下

* 回归：高斯分布
* 逻辑回归：伯努利分布
* 多类逻辑回归：分类分布
* 训练：最大化对数似然函数

### 模型非线性化、过度拟合和正则化
1. 模型非线性化

   线性回归只能对输入𝒙和输出𝑦之间的线性关系进行建模，对于线性分类器，决策边界只能是线性的如何建立具有非线性建模能力的模型？
   基本思想：利用基函数对线性模型进行非线性化。
   ![image-20250108212655250](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108212655250.png)
   ![image-20250108212730146](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250108212730146.png)

2. 过拟合

   更高维度的特征 𝝓(𝒙) 往往更适合训练数据，但高阶模型在测试数据上可能表现不佳。过拟合是指模型在训练集上表现很好，但在测试集或新数据上表现不佳的现象（模型对训练数据拟合得很好，但在未见过的数据上泛化能力差）。

   **模型在看不见的数据上表现良好的能力称为模型的泛化能力**

3. 模型选择

   如果验证误差随着模型复杂度的增加而减小，则表明模型拟合不足，否则，意味着模型拟合过度

4. 正则化（防止过拟合）

* w 很小的原因
   - 当参数 $\mathbf{w}$过大时，模型会对训练数据过于敏感，捕捉到数据中的噪声或不重要的模式，从而导致过拟合。
   - 较小的 $\mathbf{w}$ 值限制了模型的拟合能力，使其关注主要模式，而非细节噪声。
  
* 正则化通过在损失函数中加入惩罚项限制 $\mathbf{w}$ 的大小：

**$L_2$ 正则化（Ridge）**  惩罚 $\mathbf{w}$的平方和（优先缩小所有参数的绝对值，但不会完全置零)：
$$
L_Q(\mathbf{w}) = L(\mathbf{w}) + \lambda \sum w_i^2
$$

**$L_1$正则化（Lasso）**  惩罚 $\mathbf{w}$ 的绝对值和（导致一些参数值直接为零，产生稀疏解)：
$$
L_Q(\mathbf{w}) = L(\mathbf{w}) + \lambda \sum |w_i|
$$

* **但为什么不是让 $\mathbf{w}$ 直接为零？**
	- 如果所有参数都为零，模型将失去预测能力（即欠拟合）。
	- 正则化的目标是平衡复杂度与性能，完全为零会使模型失去意义。

所以，通过正则化手段，将 $\mathbf{w}$ 控制在合适的小范围内，能够在模型复杂度和性能之间找到理想的平衡点。

### 支持向量机

为什么要最大化margin：当边距较大时，我们可以预期未见过的样本有更高的机会被正确分类

**【支持向量机】**
SVM是一类按监督学习方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面，可以将问题化为一个求解凸二次规划的问题。与逻辑回归和神经网络相比，支持向量机，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。具体来说就是在线性可分时，在原空间寻找两类样本的最优分类超平面。在线性不可分时，加入松弛变量并通过使用非线性映射将低维度输入空间的样本映射到高维度空间使其变为线性可分，这样就可以在该特征空间中寻找最优分类超平面。

![image-20241103153344890](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153344890.png)

既然了解到SVM本质是为了找到一个超平面来进行二分类，接下来我们就从基本的线性分类到非线性分类以及如何周到把一个超平面来分类非线性问题并且最大化margin。

<font size = 3.5/>3.1.1线性分类的边界</font>

如图所示，对于一个线性可分的二分类问题，我们本质是找到一个可以将这个平面划分为两个部分的边界，即就是找到一条直线将点集二分类。那么问题的关键就是找到这个决策边界超平面，我们的一个做法是通过最⼩化交叉熵损失来找到决策边界超平面，定义交叉熵函数如下所示。
$$
L(\mathbf{w}, b) = -y \log \sigma(\mathbf{w}^T \mathbf{x} + b) - (1 - y) \log (1 - \sigma(\mathbf{w}^T \mathbf{x} + b))
$$

| ![image-20241103202127184](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103202127184.png) | ![image-20241103203252051](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103203252051.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

由此我们知道需要确定两个参数; $\mathbf {w}$ 和 $b$，为了找到最优的$\mathbf {w^*}$ 和 $b^*$，我们需要满足下式
$$
\mathbf{x} \cdot \mathbf{w}^* + b^* = 0
$$
由图可知我们有以下条件：超平面垂直于向量 $\mathbf{w}^* $; 从原点到平面的距离是 $ -\frac{b^*}{\|\mathbf{w}^*\|} $。其中 $ \mathbf{w} = (w_1, w_2, \ldots, w_d) $ 决定了超平面的方向，超平面垂直于 $ \mathbf{w} $，故 $ \mathbf{w} $ 的方向被称为超平面的法向量。$ b $ 是位移项，决定了超平面与原点之间的距离。显然，若点 $ \mathbf{x} $ 在超平面上，那么 $ \mathbf{w}^T \mathbf{x} + b = 0 $。如果超平面 $ (\mathbf{w}, b) $ 能将训练样本正确分类，则对于 $ (\mathbf{x}^{(l)}, y^{(l)}) \in D $，当 $ y^{(l)} = +1 $ 时有 $ \mathbf{w}^T \mathbf{x}^{(l)} + b > 0 $，当 $ y^{(l)} = -1 $ 时有 $ \mathbf{w}^T \mathbf{x}^{(l)} + b < 0 $。我们可以将上述要求合并在一起，即 $ y^{(l)} (\mathbf{w}^T \mathbf{x}^{(l)} + b) > 0 $。这里我们使用理想分类器，超平面是通过最小化损失函数确定的
$$
L(\mathbf{w}, b) = \frac{1}{N} \sum_{i=1}^{N} \max(0, 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b))
$$

- 其中 $ y \in \{-1, 1\} $
- $ e(z) = 0 $ 当 $ z \geq 0 $ 时；否则 $ e(z) = 1 $

![image-20241103204653235](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103204653235.png)

但是由于我们可以找到多对满足条件的 (w, b)。从泛化能力的角度上看，我们认为一个最优超平面应当与两个类别样本簇的间隔（margin）尽可能远，样本 𝒙 到超平面 ℋ 的距离记作：$ \mathbf{w}^\top \mathbf{x} + b = h(\mathbf{x}) $ 每个 $\mathbf{x}$ 可以分解为：$ \mathbf{x} = \mathbf{m}_\parallel + \mathbf{m}_\perp $- $\mathbf{m}_\parallel$ 在超平面 ℋ 上，即 $\mathbf{w}^\top \mathbf{m}_\parallel + b = 0$ $\mathbf{m}_\perp \perp $ ℋ 且 $\mathbf{m}_\perp \parallel \mathbf{w}$ 因此，我们有：$ \mathbf{w}^\top \mathbf{x} + b = \mathbf{w}^\top (\mathbf{m}_\parallel + \mathbf{m}_\perp) + b = \mathbf{w}^\top \mathbf{m}_\perp = h(\mathbf{x}) $ 由于 $\mathbf{m}_\perp \parallel \mathbf{w}$，我们可以写成：
$$
\mathbf{m}_{\perp} = \gamma \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|}
$$

其中 $\gamma$ 表示 $\mathbf{m}_\perp$ 的长度。

| ![image-20241103204817951](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103204817951.png) | ![image-20241103204830154](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103204830154.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

将 $\mathbf{m}_\perp = \gamma \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|}$ 代入 $h(\mathbf{x}) = \mathbf{w}^\top \mathbf{m}_\perp$ 得到：$h(\mathbf{x}) = \gamma \cdot \frac{\mathbf{w}^\top \mathbf{w}}{\|\mathbf{w}\|}$因此：$\gamma = \frac{h(\mathbf{x})}{\|\mathbf{w}\|}$样本 $\mathbf{x}$ 在上侧到超平面的距离为：$\frac{h(\mathbf{x})}{\|\mathbf{w}\|}$样本 $\mathbf{x}$ 在下侧到超平面的距离为：$-\frac{h(\mathbf{x})}{\|\mathbf{w}\|}$样本 $(\mathbf{x}, y)$ 到超平面的距离为：
$$
\frac{y \cdot h(\mathbf{x})}{\|\mathbf{w}\|} = \frac{y \cdot (\mathbf{w}^\top \mathbf{x} + b)}{\|\mathbf{w}\|}
$$
其中 $y \in \{-1, 1\}$

超平面在数据集下的间隔由最小距离给出，即：
$$
\text{Margin} = \min_{\ell} \frac{y_\ell \cdot (\mathbf{w}^\top \mathbf{x}_\ell + b)}{\|\mathbf{w}\|}
$$

因此，最大间隔分类器是找到 $\mathbf{w}^*$ 和 $b^*$ 使得它最大化间隔，即：
$$
(\mathbf{w}^*, b^*) = \arg\max_{\mathbf{w}, b} \frac{1}{\|\mathbf{w}\|} \min_{\ell} \left( y_\ell \cdot (\mathbf{w}^\top \mathbf{x}_\ell + b) \right)
$$

最后经过分析我们可以知道最大间隔分类器是找到 $\mathbf{w}^*$ 和 $b^*$ 使得它最大化间隔，即：
$$
(\mathbf{w}^*, b^*) = \arg\max_{\mathbf{w}, b} \frac{1}{\|\mathbf{w}\|} \min_{\ell} \left( y_\ell \cdot (\mathbf{w}^\top \mathbf{x}_\ell + b) \right)
$$

因此，最大间隔超平面可以通过求解以下优化问题找到：这是一个二次优化问题。其最优解可以通过数值方法高效地找到。对于最优的 $\mathbf{w}^*$ 和 $b^*$，一个未见过的数据 $\mathbf{x}$ 可以被分类为：
$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2
$$

$$
\text{s.t.} \quad y_\ell \cdot (\mathbf{w}^\top \mathbf{x}_\ell + b) \geq 1, \quad \text{for} \; \ell = 1, 2, \cdots, N
$$

$$
y(\mathbf{x}) = \text{sign}(\mathbf{w}^* \cdot \mathbf{x} + b^*)
$$

对于原始问题，分类函数为：$y(\mathbf{x}) = \text{sign}(\mathbf{w}^* \cdot \mathbf{x} + b^*)$原始问题只需要一次内积运算 $\mathbf{w}^* \cdot \mathbf{x}$

对于对偶问题，分类函数为：$y(\mathbf{x}) = \text{sign}\left( \sum_{i=1}^{N} \alpha_i^* y_i (\mathbf{x}_i \cdot \mathbf{x}) + b^* \right)$对偶问题需要 $N$ 次内积运算 $\mathbf{x}_i \cdot \mathbf{x}$，其中 $i = 1, 2, \cdots, N$

<font size = 3.5/>3.1.2非线性分类的边界</font>

| ![image-20241103211227881](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103211227881.png) | ![image-20241103211238299](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103211238299.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

为了处理这个问题，我们不再要求 $y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1$ 对于所有 $i = 1, \cdots, N$我们只要求 $y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i$，其中 $\xi_i$ 是松弛变量，且 $\xi_i \geq 0$，目标不仅是最小化 $\frac{1}{2} \|\mathbf{w}\|^2$，还需要最小化 $\sum_{i=1}^{N} \xi_i$，这导致了目标函数：
$$
\frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \xi_i
$$
其中 $C$ 用于控制相对重要性

优化问题现在变为：
$$
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \xi_i
$$

$$
\text{s.t.} \quad y_i (\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad \text{for} \; i = 1, 2, \cdots, N
$$

使用与之前相同的方法，可以推导出对偶形式：
$$
\max_{\alpha} \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^\top \mathbf{x}_j)
$$

$$
\text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^{N} \alpha_i y_i = 0
$$

当 $\alpha_i > C$ 时，可以证明 $g(\alpha) = -\infty$

对于最优的 $\mathbf{w}^*$ 和 $b^*$，一个样本 $\mathbf{x}$ 被分类为：$y(\mathbf{x}) = \text{sign}\left( \sum_{i=1}^{N} \alpha_i^* y_i (\mathbf{x}_i^\top \mathbf{x}) + b^* \right)$

**【硬间隔】**
如果样本线性可分，在所有样本分类都正确的情况下，寻找最大间隔。（如果出现异常值、或者样本不能线性可分，此时硬间隔无法实现）
![image-20241103153436692](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153436692.png)

**【软间隔和惩罚系数】**
允许部分样本，在最大间隔之内，甚至在错误的一边，寻找最大间隔。
![image-20241103153454243](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153454243.png)
我们的目标是尽可能在保持间隔宽阔和限制间隔违例之间找到良好的平衡。通过惩罚系数C来控制这个平衡：C值越小，则间隔越宽，但是间隔违例也会越多。图示的左边使用了高C值，分类器的错误样本（间隔违例)较少，但是间隔也较小。图示的右边使用了低C值，间隔大了很多，但是位于间隔上的实例也更多。

**【核函数】**
支持向量机算法分类和回归方法的中都支持线性性和非线性类型的数据类型。非线性类型通常是二维平面不可分，为了使数据可分，需要通过一个函数将原始数据映射到高维空间，从而使得数据在高维空间很容易可分，需要通过一个函数将原始数据映射到高维空间，从而使得数据在高维空间很容易区分，这样就达到数据分类或回归的目的，而实现这一目标的函数称为核函数。核函数将原始输入空间映射到新的特征空间，使得原本线性不可分的样本在核空间可分。

* 线性核：一般是不增加数据维度，而是预先计算内积，提高速度
* 多项式核：一般是通过增加多项式特征，提升数据维度，并计算内积
* 高斯核（RBF、径向基函数）：一般是通过将样本投射到无限维空间，使得原来不可分的数据变得可分。

![image-20241103153512138](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153512138.png)

![image-20241103153522427](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153522427.png)

![image-20241103153533149](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153533149.png)

由此，问题可以通过高斯核解决，其中
$$
g(\alpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j \phi(\mathbf{x}_i)^\top \phi(\mathbf{x}_j)
$$

优化问题为：
$$
\max_{\alpha} g(\alpha)
$$

$$
\text{s.t.} \quad \alpha_i \geq 0, \quad \sum_{i=1}^{N} \alpha_i y_i = 0, \quad \alpha_i \leq C
$$

分类器为：
$$
y(\mathbf{x}) = \text{sign}\left( \sum_{i=1}^{N} \alpha_i^* y_i \phi(\mathbf{x}_i)^\top \phi(\mathbf{x}) + b^* \right)
$$

**【Hinge Loss与SVM模型的关系】**
Hinge Loss 是一种常用的损失函数，特别是在支持向量机（SVM）和其他二分类任务中。它的定义主要用于评估模型的预测与真实标签之间的差距。在机器学习中，铰链损失是一个用于训练分类器的损失函数。铰链损失被用于“最大间格分类”，因此非常适合用于支持向量机 (SVM)。对于一个预期输出 $ t={\pm }1$，分类结果$y$的铰链损失定义为

Hinge Loss 的数学表达式为：
$$
\ell (y) = \max(0, 1 - t \cdot y)
$$
![image-20241103153549160](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153549160.png)*t = 1 时变量 y（水平方向）的铰链损失（蓝色，垂直方向）与0/1损失（垂直方向；绿色为 y < 0 ，即分类错误）。注意铰接损失在 abs(y) < 1 时也会给出惩罚，对应于支持向量机中间隔的概念*

特别注意：以上式子的$y$应该使用分类器的“原始输出”，而非预测标签。例如，在线性支持向量机当中，
$$
y=\mathbf {w} \cdot \mathbf {x} +b
$$
其中 $(\mathbf {w ,b})$ 是超平面参数，$ \mathbf {x} $是输入资料点。

当$ t$和$ y$同号（意即分类器的输出$ y$是正确的分类）且 $ |y|\geq 1$时，铰链损失 $ \ell (y)=0$。但是，当它们异号（意即分类器的输出$ y$是错误的分类）时，$ \ell (y)$ 随 $ y$ 线性增长。套用相似的想法，如果 $ |y|<1$，即使 $ t$ 和 $ y$ 同号（意即分类器的分类正确，但是间隔不足），此时仍然会有损失。

在SVM模型中，我们的目标是找到一个超平面，使得数据点与超平面的间隔最大化，同时尽量减少分类错误。为了实现这一目标，SVM引入了hinge loss来惩罚那些被错误分类或距离超平面太近的点。SVM的目标函数可以表示为： 
$$
min_{\mathbf{w}, b} \frac{1}{2} |\mathbf{w}|^2 + C \sum_{i=1}^{N} L(y_i, \mathbf{w}^\top \mathbf{x}_i + b)
$$
其中：
$ \frac{1}{2} |\mathbf{w}|^2 $ 是正则化项，用于最大化间隔,我们试图找到一个使得数据点与超平面的间隔最大的超平面。
(C) 是一个超参数，用于平衡正则化项和hinge loss,较大的 (C) 值会使模型更关注错误分类，而较小的 (C) 值会使模型更关注间隔的最大化。
$ L(y_i, \mathbf{w}^\top \mathbf{x}_i + b)$ 是hinge loss,确保了那些被错误分类或距离超平面太近的点会受到惩罚。

铰链损失虽然在 $ty=1$ 处不可导，但通过引入平滑变体，可以提高优化过程的稳定性。

* 铰链损失的次导数：

$$
   \frac{\partial \ell}{\partial w_i} =\begin{cases}-t \cdot x_i & \text{if } t \cdot y < 1 \\0 & \text{otherwise}\end{cases}
$$

   这表明在损失函数不为零的情况下，模型参数的梯度是非零的。

* 平滑变体：
  - 普通变体:

$$
   \ell(y) =
   \begin{cases}
   \frac{1}{2} - ty & \text{if } ty \leq 0 \\
   \frac{1}{2}(1 - ty)^2 & \text{if } 0 < ty \leq 1 \\
   0 & \text{if } 1 \leq ty
   \end{cases}
$$

   - 平方平滑变体:

$$
   \ell_{\gamma}(y) =
   \begin{cases}
   \frac{1}{2\gamma}\max(0, 1 - ty)^2 &\text{if } ty \geq 1 -\gamma \\
   1 - \frac{\gamma}{2} - ty & \text{otherwise}\end{cases}
$$

   这种平滑处理可以在优化时避免不连续性，从而更好地利用优化算法。

* Modified Huber Loss:
  当 $\gamma = 2$ 时，Modified Huber Loss 变为： 

$$
     L(t, y) = 4 \ell_2(y)
$$

* 这种损失函数结合了铰链损失和平方损失的优点，适用于不同的优化场景。

![image-20241103153609751](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153609751.png)*三个铰链损失的变体 z = ty：“普通变体”（蓝色），平方变体（绿色），以及 Rennie 和 Srebro 提出的分段平滑变体（红色)*

**【Cross-entropy Loss】**
交叉熵源于信息论，衡量了两个概率分布之间的差异。具体来说，它衡量的是使用一个概率分布（模型预测的分布）来编码另一个分布（真实标签）的信息量。通过最小化交叉熵损失，我们可以使模型的输出概率分布尽可能接近真实分布。

对于二分类问题，交叉熵损失的公式为：
$$
L(y, \hat{y}) = -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
$$

- $y$ 是真实标签（0或1）。
- $\hat{y}$ 是模型对样本为正类的预测概率（通常通过 sigmoid 函数获得）。

当 $y=1$ 时，损失为 
$$
-\log(\hat{y})
$$
当 $y=0$ 时，损失为 
$$
-\log(1 - \hat{y})
$$

| ![image-20241103213332849](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103213332849.png) | ![image-20241103213346232](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103213346232.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20241103153630524](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153630524.png) | ![image-20241103153642810](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241103153642810.png) |

### 神经网络

1. 许多应用都需要富有表现力的非线性模型现有的非线性化方法
   a) 特征变换，基函数 𝒙 ⟶ 𝝓(𝒙)
   b) 核方法，可以理解为用无限维基函数 𝝓 ⋅ 表示的变换
   c) 神经网络

2. 激活函数

   ![image-20250109092831363](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250109092831363.png)

**【多层感知机（MLP）】**

单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。一个经典的神经网络包含三个层次：红色的是 输入层 ，绿色的是 输出层 ，紫色的是 中间层 （也叫 隐藏层 ）– 输入层与输出层的节点数往往是固定的，中间层则可以自由指定– 结构图里的关键不是圆圈（代表“神经元”），而是连接线。每个连接线对应一个不同的权重。

| ![image-20241123172342234](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123172342234.png) | ![image-20241123172247825](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123172247825.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

我们可以看到输出层的决策分界仍然是直线。关键是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。

| ![image-20241123172511836](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123172511836.png) | ![image-20241123172439877](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123172439877.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 决策边界                                                     | 空间变化                                                     |

**多层感知器－训练**

* 梯度下降 算法：每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

* 反向传播 算法：反向传播算法不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

  ![image-20241123172734301](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123172734301.png)
  

在实际训练过程中，需要先进行正向传播训练，再进行反向传播训练

* 正向传播（Forward Propagation）
正向传播是从输入数据通过网络层层传递并计算输出的过程。输入数据作为特征向量，进入网络。对于隐藏层，每一层都有权重矩阵 $ W $ 和偏置向量 $ b $；对于每一个神经元，其输入信号是前一层输出的加权和，加上偏置；每一层的输出可以表示为：
$$
z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}
$$
其中，$ l $ 表示当前层数，$ a^{(l-1)}$ 是上一层的激活输出，$ z^{(l)} $ 是当前层的线性组合输出。然后，对 $ z^{(l)} $ 进行非线性激活函数的处理，得到当前层的输出：$a^{(l)} = \sigma(z^{(l)})$,其中，$ \sigma $ 是激活函数，如 Sigmoid、ReLU、Tanh 等;最后一层通常会有一个特定的激活函数（如 Softmax）来生成最终的预测结果。通过正向传播，可以得到网络的输出（预测值）。

* 反向传播
  ![image-20241123173206723](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241123173206723.png)
  反向传播是用于更新权重和偏置的关键算法。目标是通过计算损失函数的梯度，调整网络参数，使损失函数值减小。步骤如下：从输出层开始，计算损失对每一层参数（权重和偏置）的梯度，通过链式法则，将梯度从输出层传递回隐藏层和输入层，对于输出层的梯度：
$$
\delta^{(L)} = \nabla_a \mathcal{L} \odot \sigma'(z^{(L)})
$$
其中，$ \delta^{(L)} $ 是输出层的误差，$ \nabla_a \mathcal{L} $ 是损失对输出的导数，$ \sigma'(z^{(L)}) $ 是激活函数的导数。对于隐藏层的梯度：
$$
\delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})
$$
递推公式依次向前层传递，计算各层的误差。最后计算权重和偏置更新：使用计算出的梯度，更新每一层的参数： 
$$
W^{(l)} = W^{(l)} - \eta \cdot \frac{\partial \mathcal{L}}{\partial W^{(l)}}
$$
$$
b^{(l)} = b^{(l)} - \eta \cdot \frac{\partial \mathcal{L}}{\partial b^{(l)}}
$$
其中，$ \eta $ 是学习率。

**【卷积神经网络（CNN）】**

![img](https://i-blog.csdnimg.cn/blog_migrate/3c266da23107494b04b09683b8427f0e.png)

主要由以下5层组成：

i.数据输入层Input layer —— 对原始图像数据进行预处理

| ![img](https://pic1.zhimg.com/v2-4c3b00f07cce0c7dccf2e4ba5e167e30_r.jpg) | ![img](https://picx.zhimg.com/v2-229a2c9828a26d594dc854b659cfc8a5_r.jpg) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 去均值与归一化效果图                                         | 去相关与白化效果图                                           |

去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。
归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，通过归一化将A和B的数据都变为0到1的范围。(PCA白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化)

ii.卷积计算层CONV layer —— 提取特征

![img](https://pic4.zhimg.com/v2-50e4ca38e42aa9f91e5419a107b76a07_r.jpg)

输入矩阵格式：四个维度，依次为：样本数、图像高度、图像宽度、图像通道数
输出矩阵格式：与输出矩阵的维度顺序和含义相同，但是后三个维度（图像高度、图像宽度、图像通道数）的尺寸发生变化。
权重矩阵（卷积核）格式：同样是四个维度，但维度的含义与上面两者都不同，为：卷积核高度、卷积核宽度、输入通道数、输出通道数（卷积核个数）
输入矩阵、权重矩阵、输出矩阵这三者之间的相互决定关系
卷积核的输入通道数（in depth）由输入矩阵的通道数所决定。
输出矩阵的通道数（out depth）由卷积核的输出通道数所决定。
输出矩阵的高度和宽度由输入矩阵、卷积核（过滤器）的尺寸、步幅（stride）、填充（padding）共同决定。

![img](https://picx.zhimg.com/v2-3374c724f32488eb8e8552e1b9661d99_r.jpg)

设：
- 输入矩阵的高度为 $ H_{in} $，宽度为 $ W_{in} $
- 卷积核的高度为 $ H_{f} $，宽度为 $ W_{f} $
- 填充的高度为 $ P $，宽度为 $ P $（假设填充在高度和宽度方向相同）
- 步幅为 $ S $

则输出矩阵的高度 $ H_{out} $ 和宽度 $ W_{out} $ 计算公式为：

$ H_{out} = \left\lfloor \frac{H_{in} - H_{f} + 2P}{S} \right\rfloor + 1 $

$ W_{out} = \left\lfloor \frac{W_{in} - W_{f} + 2P}{S} \right\rfloor + 1 $

iii.ReLU激励层ReLU layer —— 引入非线性特征

| ![img](https://pic3.zhimg.com/v2-4f12096f7b6fb83ce6dc96b3ecf915c8_r.jpg) | ![img](https://pic2.zhimg.com/v2-a559927aa4df378c6b1a25c2cb86db5b_r.jpg) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

在每个卷积层之后，通常会立即应用一个非线性层（或激活层）。其目的是给一个在卷积层中刚经过线性计算操作（只是数组元素依次（element wise）相乘与求和）的系统引入非线性特征。它同样能帮助减轻梯度消失的问题——由于梯度以指数方式在层中消失，导致网络较底层的训练速度非常慢。
ReLU 层对输入内容的所有值都应用了函数 f(x) = max(0, x)。用基本术语来说，这一层把所有的负激活（negative activation）都变为零。这一层会增加模型乃至整个神经网络的非线性特征，而且不会影响卷积层的感受野。由于越靠近1表示与该特征越关联，越靠近-1表示越不关联，而我们进行特征提取时，为了使得数据更少，操作更方便，就直接舍弃掉那些不相关联的数据。>=0的值不变，而<0的值一律改写为0

iv.池化层Pooling layer —— 用于对输入特征图进行降维和特征提取

![img](https://pic2.zhimg.com/v2-deeacf1fc2ef42c0e41070fae4fb5381_r.jpg)

在ReLU 层之后，会选择使用用一个池化层，用于对输入特征图进行降维和特征提取。
1.降维：通过减少特征图的空间尺寸（高度和宽度），降低计算复杂度和内存使用，防止过拟合。
2.保留重要特征：池化操作保留输入特征图中的主要特征，同时丢弃次要的特征。这有助于模型更好地提取重要的信息。

3. 提高不变性：池化层可以增强特征图的平移不变性，即在输入图像发生小的平移时，输出特征图变化不大。这提高了模型对位置变化的鲁棒性。

最大池化（Max Pooling）

通过在特征图的局部区域（通常是一个小的矩形窗口）内取最大值来进行降维。假设窗口大小为 $2 \times 2$，步幅为 $2$，则每个 $2 \times 2$ 的窗口中的最大值将被保留，其余值被丢弃。

通过以上的池化操作，我们减少了特征图的尺寸，同时保留了输入特征图的主要信息。

*到这里就介绍了CNN的基本配置---卷积层、Relu层、池化层。在常见的几种CNN中，这三层都是可以堆叠使用的，将前一层的输入作为后一层的输出.*

v.全连接层FC layer —— 综合提取的特征进行分类或回归任务

![img](https://pic3.zhimg.com/v2-9cbccaabf38a4c5c4c8494afc3556c12_r.jpg)

两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的.
全连接层中的函数----Softmax，它是一个分类函数，输出的是每个对应类别的概率值.

vi.Dropout —— 正则化技术，旨在防止神经网络过拟合

训练阶段：在每个训练迭代中，对于每一层的每个神经元，以概率 𝑝临时将其“丢弃”（即将该神经元的输出设为零）。保留下来的神经元的输出除以 1 − 𝑝 进行缩放，以保持输入的期望值不变。
测试阶段：在测试过程中，所有神经元都被激活，但不进行任何丢弃操作。

### 优化器

**【SGD算法、SGD Momentum算法和Adam算法】**

**SGD算法（Stochastic Gradient Descent）**

SGD是一种最常用的优化算法，它是传统梯度下降算法的变体。标准的梯度下降算法每次计算损失函数关于模型参数的**全部数据**的梯度，然后根据这个梯度更新参数。相对而言，SGD每次仅计算**一个样本**（或一小部分样本，即批量梯度下降中的mini-batch）上的梯度，从而加速了计算过程。

SGD的更新规则：
$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t; x^{(i)}, y^{(i)})
$$
其中：
- $\theta$ 是模型的参数。
- $\eta$ 是学习率。
- $\nabla_\theta L(\theta_t; x^{(i)}, y^{(i)})$ 是当前样本$(x^{(i)}, y^{(i)})$上的梯度。

虽然SGD计算速度快，每次更新使用单个样本，适合大规模数据集，但是更新过程比较噪声大（因为使用单个样本的梯度），导致收敛速度较慢，甚至可能无法收敛，难以调整到全局最优解，由此引入了SGDM算法。

**SGD Momentum算法**

Momentum（动量）算法是对标准SGD的改进，它通过引入“动量”的概念来加速收敛并减少振荡。Momentum算法考虑到过去更新的方向，使得当前的更新不仅仅依赖于当前梯度，还会依赖于上次的更新方向，从而使参数更新过程更平滑、更稳定。

SGD Momentum的更新规则：
$$
v_{t+1} = \beta v_t + (1 - \beta) \nabla_\theta L(\theta_t)
$$
$$
\theta_{t+1} = \theta_t - \eta v_{t+1}
$$
其中：
- $v_t$ 是动量项，表示当前梯度的累积值。
- $\beta$ 是动量因子（通常设为接近1，比如0.9），控制了过去梯度的影响程度。
- 其余符号和SGD一致。

**Adam算法（Adaptive Moment Estimation）**

Adam（自适应矩估计）是一种结合了动量和自适应学习率的优化算法。它不仅考虑梯度的历史信息，还对每个参数的学习率进行自适应调整。Adam对每个参数的梯度进行两种不同的估计：
- 一阶矩（梯度的平均值），类似于动量。
- 二阶矩（梯度的方差），用于调整每个参数的学习率。

Adam的更新规则：
$$
m_{t+1} = \beta_1 m_t + (1 - \beta_1) \nabla_\theta L(\theta_t)
$$
$$
v_{t+1} = \beta_2 v_t + (1 - \beta_2) (\nabla_\theta L(\theta_t))^2
$$
$$
\hat{m}_{t+1} = \frac{m_{t+1}}{1 - \beta_1^{t+1}}, \quad \hat{v}_{t+1} = \frac{v_{t+1}}{1 - \beta_2^{t+1}}
$$
$$
\theta_{t+1} = \theta_t - \eta \frac{\hat{m}_{t+1}}{\sqrt{\hat{v}_{t+1}} + \epsilon}
$$
其中：
- $m_t$ 是梯度的一阶矩（动量）。
- $v_t$ 是梯度的二阶矩（梯度的方差）。
- $\beta_1$ 和 $\beta_2$ 是控制一阶矩和二阶矩的衰减率，通常设置为接近1（例如0.9和0.999）。
- $\epsilon$ 是一个很小的常数，用来避免除零错误（通常设为$10^{-8}$）。


| 特性           | **SGD**                | **SGD Momentum**                 | **Adam**                               |
| -------------- | ---------------------- | -------------------------------- | -------------------------------------- |
| **更新规则**   | 仅使用当前梯度进行更新 | 结合当前梯度和之前的梯度（动量） | 使用梯度的一阶和二阶矩，进行自适应调整 |
| **收敛速度**   | 相对较慢，容易震荡     | 加速收敛，减少震荡               | 快速收敛，特别是在复杂问题中           |
| **计算开销**   | 低                     | 稍高，但仍然较低                 | 相对较高，需要维护动量和方差估计       |
| **适应性**     | 固定学习率             | 动量帮助稳定收敛                 | 自适应学习率，更灵活且适用广泛         |
| **超参数调整** | 学习率需要调节         | 学习率和动量因子需要调节         | 学习率和$\beta$值需要调节              |

- **SGD**简单但容易陷入局部最优解或收敛慢。
- **SGD Momentum**通过引入动量来加速收敛并减少震荡，适用于大部分任务。
- **Adam**结合了动量和自适应学习率，通常在大规模深度学习任务中表现出色，但可能需要更多的超参数调整。

### 数据预处理

1. 常用的数据预处理方法：
   1）对数据进行中心化，即减去数据均值
   2）对每个维度的方差进行归一化
   3）对数据进行白化
   根据数据的特点，我们可以使用上述一种或多种方法对数据进行预处理

   ![image-20250109094036512](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250109094036512.png)

2. dropout正则化，去除过拟合现象

   在每次前向传递中，随机将一些神经元设置为零，Dropout 可以有效缓解深度神经网络中常见的过拟合问题。

3. 超参数调整

   在开始训练之前，需要设置很多参数，例如学习率、RMSprop 和 Aadm 中的衰减参数、dropout 率等，要为这些超参数设置适当的值，我们需要将整个数据集分成三个部分，即训练集、验证集和测试集。
   在训练集上训练模型；根据验证集上的表现调整超参数；测试集上测试性能，并将其报告为最终性能。
   超参数不是盲目调整的，而是可以根据一些指导原则进行选择：首先尝试一些在许多任务上表现良好的默认值；根据训练期间观察到的现象选择值。

### 决策树

树不能永远扩大，应该在某个点停止。有一些停止标准，如下所述
Ø 所有剩余实例都有相同的标签
Ø 我们用完了所有属性
Ø 树的深度达到最大限制
Ø 信息增益小于阈值

  

为了控制决策树的复杂性，我们可以
Ø 在学习树时修剪分支
Ø 在学习树后修剪分支
• 基于验证数据集，
Ø 修剪不会损害验证集准确性的节点
Ø 贪婪地删除对验证准确性提高最小的节点
Ø 当验证集准确性开始下降时停止



### Bagging & Boosting

很难学习一个总能正确分类实例的强分类器
•但很容易学习很多“弱”分类器
一个弱分类器可能在整个数据集上表现不佳，但可能在一小部分样本上表现良好，例如，一些可能擅长识别“猫”，而另一些可能擅长识别“狗”
•如果弱分类器在不同部分的样本上表现良好，那么可以通过以适当的方式组合这些弱分类器来得出强分类器
•两个问题

1) 如何生成这些弱分类器？
2) 如何组合弱分类器？

* 未加权平均——多数投票——baggIng

  导出弱分类器，我们不知道如何获得在不同样本部分上表现良好的分类器，相反，我们尝试获得产生独立预测误差的分类器，即鼓励它们的预测不相关。例如







* 加权平均——给予更好的分类器更大的权重——boosting
  例如，考虑一个二分类问题 {-1, 1}
  ![image-20250109224907946](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250109224907946.png)
  备注：弱分类器可以是任何类型，例如决策树、
  SVM、神经网络、逻辑回归等

### PCA

PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。

**思考：**我们如何得到这些包含最大差异性的主成分方向呢？

**答案：**事实上，通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。

由于得到协方差矩阵的特征值特征向量有两种方法：特征值分解协方差矩阵、奇异值分解协方差矩阵，所以PCA算法有两种实现方法：基于特征值分解协方差矩阵实现PCA算法、基于SVD分解协方差矩阵实现PCA算法。

![image-20250110201928734](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250110201928734.png)

**左奇异矩阵可以用于对行数的压缩；右奇异矩阵可以用于对列(即特征维度)的压缩**

![image-20250110202109312](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20250110202109312.png)

### 无监督

**【K-Means】**

| ![image-20241205165410118](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241205165410118.png) | ![image-20241205165450549](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241205165450549.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

*k-means*的目的是：把n个点（可以是样本的一次观察或一个实例）划分到*k*个聚类中，使得每个点都属于离他最近的均值（此即聚类中心)对应的聚类，以之作为聚类的标准。已知观测集${\displaystyle (x_{1},x_{2},...,x_{n})}$，其中每个观测都是一个${\displaystyle d}$-维实向量，k-均值聚类要把这${\displaystyle n}$个观测划分到k个集合中(k≤n),使得组内平方和（WCSS within-cluster sum of squares）最小。换句话说，它的目标是找到使得下式满足的聚类${\displaystyle S_{i}}$，
$$
{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}}
$$
其中${\displaystyle \mu _{i}}$是${\displaystyle S_{i}}$中所有点的均值。

---

**【K-Means 算法步骤】**

1. **问题定义**：
   - 数据矩阵 $ A \in \mathbb{R}^{N \times P} $，每一行表示一个数据点，总共有 $ N $ 个点，每个点的维度是 $ P $。
   - 将 $ A $ 分成 $ K $ 个簇，通过最小化以下目标函数实现：
   - $ \Phi \in \mathbb{R}^{N \times K} $ 是分配矩阵，每行只有一个分量为 1，其余为 0，表示每个点分配到一个簇。
   - $ H \in \mathbb{R}^{K \times P} $ 是簇中心矩阵，每行对应一个簇中心。
$$
\min \| A - \Phi H \|_F^2
$$

2. **目标函数的两部分**：
   - 固定 $ \Phi $，更新 $ H $：计算每个簇的中心。
   - 固定 $ H $，更新 $ \Phi $：根据当前簇中心重新分配点。

---

**初始化**
随机初始化 $ \Phi $ 或 $ H $：

- 初始化 $ H $：随机选取 $ K $ 个点作为初始簇中心。
- 或初始化 $ \Phi $：随机分配每个点到某个簇。

**迭代过程**
**1. 更新 $ H $（计算簇中心）**
对于每个簇 $ j $，计算簇内样本点的均值，更新簇中心：
$$
h_j = \frac{\sum_{\mathbf{x}_i \in C_j} \mathbf{x}_i}{|C_j|}
$$
在矩阵形式下：
$$
H = \arg \min_H \| A - \Phi H \|_F^2
$$
这是因为每个簇中心是对应簇内点到中心距离平方和最小化的结果。

**2. 更新 $ \Phi $（重新分配点）**
对于每个点 $ \mathbf{x}_i $，找到距离最近的簇中心 $ h_j $，并将点重新分配到对应的簇：
$$
\phi_{ij} = 
\begin{cases} 
1 & \text{if } j = \arg\min_k \| \mathbf{x}_i - h_k \|^2 \\
0 & \text{otherwise}
\end{cases}
$$

换句话说：
- 对于每个点，计算它与所有簇中心的距离。
- 分配到使距离最小的那个簇。

**收敛条件**
重复更新 $ \Phi $ 和 $ H $，直到以下条件之一满足：

- 簇中心 $ H $ 不再变化（或变化小于某一阈值）。
- 最大迭代次数达到。

---

注意：
- **$ \Phi $ 的性质**：每行只有一个分量为 1，表示点的分配。
- **目标分解**：
  - 分解为 $ \min_H $ 和 $ \min_\Phi $ 两部分。
- **更新公式**：
  - 分配矩阵更新通过最小距离选择。
  - K-Means 的核心思想就是交替优化分配矩阵 $ \Phi $ 和簇中心矩阵 $ H $。

![image-20241205163541561](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241205163541561.png)

1. 选择初始化的 k 个样本作为初始聚类中心 $a=a_1,a_2,…a_k$ ；
2. 针对数据集中每个样本 $x_i$ 计算它到 $k$ 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；
3. 针对每个类别 $a_j$ ，重新计算它的聚类中心 $a_j = \frac{1}{|ci|}∑_{x∈ci}x$ （即属于该类的所有样本的质心）；
4. 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。

---

**【GMM】**

![image-20241206104854350](C:\Users\14255\AppData\Roaming\Typora\typora-user-images\image-20241206104854350.png)

高斯分布，又叫正态分布（Normal Distribution），是最最重要的概率分布。原因就是因为它在生活和工程中都非常常见。它的概率密度函数（PDF，probability density function）中间高两边低，且关于均值对称。高斯混合模型（**G**aussian **M**ixed **M**odel）简称GMM，指多个单高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布。高斯混合模型通常用于解决同一集合下的数据包含多个不同的分布的情况，具体应用有聚类、密度估计、生成新数据等。严格来说，GMM 不是一种聚类算法，而只是一种用来拟合数据分布的模型。然而，我们可以假设 每个簇的样本都符合一个高斯分布。这样，我们可以使用训练集的样本数据来训练、拟合一个 GMM 模 型。通过计算某个样本数据中各个高斯分布的贡献比例，我们就可以推断这个样本数据所属的簇。某个 簇的高斯分布对这个样本数据的贡献越大，那么该样本就越可能属于这个簇。

| ![img](https://pic3.zhimg.com/v2-aab13cef5d9e89f633f93959570e4efc_r.jpg) | ![img](https://pic2.zhimg.com/v2-56468e3d04d84e13ca3376a86d8a6af5_r.jpg) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 一维GMM                                                      | 二维GMM                                                      |

![img](https://pica.zhimg.com/v2-a8f4f1a0aba13886275a4365da6e716c_r.jpg)

GMM是使用多个高斯分布来进行建模的。每一个高斯分布都未知（均值方差都未知），每个高斯对整体GMM的贡献比例也未知。那么采集一组数据，想要知道每一个数据出自哪一个高斯，就得采用迭代的方法来计算该数据出自某一个高斯的概率: 比如第一个数据有可能出自第一个高斯，也有可能出自第二个高斯，也有可能出自第三个高斯..., 这时就得用迭代的算法。上面的全概率公式中，P(A)就表示第一个高斯的概率分布，P(B)表示第二个高斯的概率分布，P(C)表示第三个高斯的概率分布。P(D|A)表示在第一个高斯中取得数据D的概率，P(D|B)表示在第二个高斯中取得数据D的概率，P(D|C)表示在第三个高斯中取得数据D的概率。P(D)表示取得整体数据D的概率。而公式中的参数theta，在GMM中是一个向量，表示的是各个高斯的均值和方差，以及每个高斯对整体GMM的贡献比例lambda。由此，GMM就和全概率公式和贝叶斯公式扯上了关系。

先简单地回顾一下多元高斯分布的定义。对于 $n$ 维样本空间 $X$ 中的随机向量 $x$ ，若其概率密度函数为：
$$
p(\boldsymbol{x}) = \frac{1}{(2\pi)^{n/2} (\det \Sigma)^{1/2}} \exp \left\{ - \frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^{\mathsf{T}} \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) \right\}
$$

则称 $x$ 服从高斯分布。其中 $μ$ 是 $n$ 维均值向量， $Σ$ 是$ n×n $的协方差矩阵， $detΣ$ 是方阵 $Σ$ 的行列式。从上面式子可知，高斯分布完全由均值向量 $μ$ 和协方差矩阵$ Σ $这两个参数确定。为了明确显示高斯分布与相应参数的依赖关系，常常将高斯分布的概率密度函数$ p(x)$ 记为 $p(x∣μ,Σ)$ 或者 $N(x∣μ,Σ)$ 。

![img](https://pic1.zhimg.com/v2-d9598f15bb2b732a7361ad8a7a8b45be_r.jpg)

高斯混合模型（GMM）的数学表达式：

$$
GMM = \sum_{k} \lambda_k \mathcal{N}(\mu_k, \sigma_k)
$$

其中各符号的含义如下：

- $\lambda_k$：混合系数，表示第 $k$ 个高斯分布在总体中的权重，满足 $\lambda_k > 0$ 且 $\sum_{k} \lambda_k = 1$。
- $\mathcal{N}(\mu_k, \sigma_k)$：第 $k$ 个高斯分布的概率密度函数，其均值为 $\mu_k$，标准差为 $\sigma_k$（对于多维情况，则是均值向量和协方差矩阵）。
- $k$：表示第 $k$ 个高斯分布，总共 $K$ 个分布。

**GMM 用作聚类算法的思想很简单：假设样本数据服从混合高斯分布，根据样本数据集推出混合高斯分布的各个参数，以及各个样本最可能属于哪个高斯分布，这样，GMM 的 K （ K 值往往由用户提供）个成分对应于聚类任务中的 K 个簇，每个样本划到最可能的高斯分布所对应的簇中。**由于GMM 是一种**生成模型**，可以理解为以下随机过程生成数据：随机从 $ K $个高斯分布中选择一个 $ k $（根据混合系数 $ \pi_k $）；从选定的第 $ k $个高斯分布 $ \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) $中生成一个数据点。换句话说：**每个数据点的生成有“隐变量” $ z $，表示数据属于哪个高斯分布**。$ z $是一个离散变量，取值范围为 $ \{1, 2, \dots, K\} $。因此隐变量的概率分布为：$p(z = k) = \pi_k, \quad k \in \{1, 2, \dots, K\}$，数据点 $ \mathbf{x} $的条件分布为：$p(\mathbf{x} \mid z = k) = \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$，因此，联合分布可以写为：$p(\mathbf{x}, z) = p(z) \cdot p(\mathbf{x} \mid z)$，对于观测数据 $ \mathbf{x} $，隐变量 $ z $被积分掉后，得到边缘分布：
$$
p(\mathbf{x}) = \sum_{k=1}^K p(z = k) \cdot p(\mathbf{x} \mid z = k) = \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
$$

其中：
- $ K $：高斯分布的数量（簇的数量）。
- $ \pi_k $：第 $ k $个高斯分布的混合系数，满足 $ \pi_k > 0 $且 $ \sum_{k=1}^K \pi_k = 1 $。
- $ \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) $：第 $ k $个高斯分布的概率密度函数：
$$
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) = \frac{1}{(2\pi)^{d/2} |\boldsymbol{\Sigma}_k|^{1/2}} \exp\left(-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_k)^\top \boldsymbol{\Sigma}_k^{-1} (\mathbf{x} - \boldsymbol{\mu}_k)\right)
$$
- $ \boldsymbol{\mu}_k $：第 $ k $个高斯分布的均值（簇中心）。
- $ \boldsymbol{\Sigma}_k $：第 $ k $个高斯分布的协方差矩阵。

假设样本的生成过程由高斯混合分布给出。高斯混合分布的抽样过程如下：首先，随机地在 $K$ 个高斯混合成分之中选一个，每个成分被选中的概率是它的系数 $πk$ ；然后，再单独考虑这个被选择的混合成分，根据它的概率密度函数进行采样，从而生成相应的样本。若样本集 $D={x1,x2,⋯,xN}$ 由上述过程生成，如何通过样本集估计出模型参数 ${(πk,μk,Σk)∣1≤k≤K}$ 呢？显然，对于给定的样本集 $D$ ， **如果知道每个样本点是由哪个高斯混合成分生成，则直接采用“极大似然估计”即可求解参数，** 即最大化对数似然：**目标**：通过 $ K $个高斯分布拟合数据点 $ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N $。GMM 的目标是最大化对数似然函数：
$$
\mathcal{L} = \log \prod_{i=1}^N p(\mathbf{x}_i) = \sum_{i=1}^N \log \left( \sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)
$$

问题在于，对于这个数据集 D 我们并不知道每个样本点是由哪个高斯混合成分生成（这称为“不完全数据集”），从而无法直接使用极大似然估计来求解参数。这种情况下，我们常用 EM 算法来迭代优化求解。因此引入隐变量 $ z $的后验概率 $ \gamma_{ik} $来辅助计算。

（**E步**）我们引入一个 K 维随机变量 z （通常称之为隐含变量，Latent Variable）来描述“样本点是由哪个高斯混合成分生成”这个未知事情，它满足条件：$ zk(1≤k≤K)$ 只能取 0 或者 1；且 $∑k=1Kzk=1 $。例如，样本集共有 3 个类别，某一样本点如果它由第 1 个高斯混合成分生成，则相应随机变量 $z=(1,0,0)$ ，如果它由第 2 个高斯混合成分生成，则相应随机变量 z=(0,1,0) ，依此类推 。这样，$ P(zk=1) $可表示当前样本点由第 k 个高斯混合成分生成的概率。显然有， $P(zk=1)=πk$ 。**给定样本点 $xi $，它由第 k 个高斯混合成分生成的概率（后验概率）为 $pM(zk=1∣xi) $，为方便起见把它简记为$ γik$** ，由贝叶斯公式和全概率公式知：计算隐变量的后验概率（即数据点属于第 $ k $个高斯分布的概率）,其中$ \gamma_{ik} $是软分配概率，表示 $ \mathbf{x}_i $属于第 $ k $个簇的概率。但实际上我们更多将$ \gamma_{ik} $定义成当第 $x_i$ 来自第 $k$ 个高斯分量时$ \gamma_{ik} = 1$，否则$ \gamma_{ik} = 0$。
$$
\begin{align*} \color{red}{\gamma_{ik}} & \triangleq p_M(z_k = 1 \mid \boldsymbol{x_i}) \\
& = \frac{P(z_k = 1) \cdot p_M(\boldsymbol{x_i} \mid z_k = 1)}{p_M(\boldsymbol{x_i})} \\
& = \frac{P(z_k = 1) \cdot  p_M(\boldsymbol{x_i} \mid z_k = 1)}{\sum_{j=1}^{K} P(z_j = 1) \cdot p_M (\boldsymbol{x_i} \mid z_j = 1)} \\
& = \frac{\pi_k \cdot p_M(\boldsymbol{x_i} \mid z_k = 1)}{\sum_{j=1}^{K} \pi_j \cdot p_M (\boldsymbol{x_i} \mid z_j = 1)} \\
& \color{red}{= \frac{\pi_k \cdot \mathcal{N}(\boldsymbol{x_i} \mid \boldsymbol{\mu_k}, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \cdot \mathcal{N}(\boldsymbol{x_i} \mid \boldsymbol{\mu_j}, \Sigma_j)}}  \tag{EM算法中“E step”} \\
\end{align*}
$$

（**M步**）利用 E 步的后验概率 $ \gamma_{ik} $，更新模型参数 $ \pi_k $、$ \boldsymbol{\mu}_k $、$ \boldsymbol{\Sigma}_k $：
$$
\pi_k = \frac{\sum_{i=1}^N \gamma_{ik}}{N}
$$
$$
\boldsymbol{\mu}_k = \frac{\sum_{i=1}^N \gamma_{ik} \mathbf{x}_i}{\sum_{i=1}^N \gamma_{ik}}
$$
$$
\boldsymbol{\Sigma}_k = \frac{\sum_{i=1}^N \gamma_{ik} (\mathbf{x}_i - \boldsymbol{\mu}_k)(\mathbf{x}_i - \boldsymbol{\mu}_k)^\top}{\sum_{i=1}^N \gamma_{ik}}
$$

（**重复**）作为隐变量，$ \gamma_{ik} $是不可知的。虽然不可知，但我们可以根据当前的样本数据和 $GMM $参数 “猜测”$ \gamma_{ik} $的值，再基于 “猜测值” 反过来估计 $GMM$ 的参数。这就是 EM 算法的思想。在 E 步与 M 步之间交替迭代，直到参数收敛（对数似然函数不再显著增加）。

**当迭代结束后，样本点 $xi$ 所属的类别 $λi$ 由下式确定**
$$
\lambda_i = \underset{k \in \{1,2,\cdots, K \}}{\operatorname{arg} \max} \gamma_{ik}
$$

---

**【EM】**

EM算法用来解决含有隐变量（latent variable(s)）的估计问题。EM算法就是构造一个期望(数学上，期望就是平均值)，然后在此基础上进行操作，使得概率变得最大。主要用于解决含有隐变量的概率估计问题。在高斯混合模型（GMM，Gaussian mixture module），隐马尔可夫模型（HMM，Hidden Markov Model）等数学模型中都有涉及。EM算法是一个迭代算法，每一次迭代都会分为2个阶段：E步（求期望）和M步（求最大）。

给定数据集 $ X = \{x_1, x_2, \dots, x_N\} $，我们假设它来自一个概率分布，但这个分布依赖于一些无法观测的隐变量 $ Z = \{z_1, z_2, \dots, z_N\} $。目标是最大化观测数据的对数似然函数：
$$
\log p(X|\theta) = \log \int_Z p(X, Z|\theta) dZ
$$
其中：
\- $ \theta $ 是模型的参数。
\- $ Z $ 是隐变量（例如在GMM中，隐变量是样本属于哪个高斯分布的类别标记）。

但是直接优化 $ \log p(X|\theta) $ 通常很难，因为积分或求和计算复杂。因此，EM通过引入**隐变量的分布 $ q(Z) $** 来转化为更容易处理的形式。

**EM算法的核心思想：Lower Bound 与 KL 散度**

**Jensen不等式**是 EM 的核心：通过引入分布 $ q(Z) $，我们将 $\log p(X|\theta)$ 的优化转化为两项的求和：
$$
\log p(X|\theta) \geq \mathcal{L}(q, \theta) = \int q(Z) \log \frac{p(X, Z|\theta)}{q(Z)} dZ
$$

其中：
- $\mathcal{L}(q, \theta)$：称为 Lower Bound，表示 $ \log p(X|\theta) $ 的下界。
- $ KL(q || p) = \int q(Z) \log \frac{q(Z)}{p(Z|X, \theta)} dZ $：是 KL 散度，度量 $ q(Z) $ 与真实后验分布 $ p(Z|X, \theta) $ 的差异。

由于 $ \log p(X|\theta) = \mathcal{L}(q, \theta) + KL(q || p) $，最大化 $\mathcal{L}(q, \theta)$ 等价于最大化 $\log p(X|\theta)$。

* **E 步（Expectation）：固定 $ \theta $，优化 $ q(Z) $**，选择使 KL 散度最小的 $ q(Z) $，即让 $ q(Z) = p(Z|X, \theta) $。因此：
$$
q(Z) = p(Z|X, \theta^{(t)})
$$
这一步是计算隐变量的后验分布。

* **M 步（Maximization）：固定 $ q(Z) $，优化 $ \theta $**将 ，$ q(Z) $ 代入 Lower Bound，并最大化：
$$
\theta^{(t+1)} = \arg\max_{\theta} \mathcal{L}(q, \theta)
$$

![gmm.jpg](https://aandds.com/blog/images/gmm.jpg)



### 推荐系统：基于内容的推荐和协同滤波

* **推荐系统问题**：推荐系统问题旨在用户推荐相关项，项可以是用户未观看过的电影、书籍，未访问过的网站，可以是任何可以购买的产品，实现一种个性化的推荐。推荐系统可以总结为以下模型：
  $$
  \text{Utility Function: } u: X \times S \to R
  $$
  其中，$X$ 是用户的集合，$S$ 是项的集合，$R$ 是用户对项评分的集合，并且是关于项的有序集。

  * 推荐系统问题关键问题：如何为矩阵收集已知的评级，如何从已知的评级中推断未知的评级，如何评估推断的好坏。

  * 冷启动问题：项没有被评分，用户的评分记录为空。

* **基于内容的推荐系统**：**向客户推荐与用户之前评价较高的产品相似的产品**

  * **跟据用户对项的评分建立项的画像（特征向量），在建立用户画像（特征向量），根据用户画像推荐项（计算相似度）。**

  * **项的画像**：**一个特征向量**

    * 例如文本挖掘使用TF-IDF，即项的频率乘以逆文档频率
      $$
      f_{ij} = \text{项 i 在文档 j 的出现频率} \\
      TF_{ij} = \frac{f_{ij}}{\max_k f_{kj}} \\
      n_i = \text{出现项 i 的文档的数目} \\
      IDF_i = \log\frac N{n_i} \\
      w_{ij} = TF_{ij}\times IDF_{i}
      $$

  * **用户画像**：例如评分项画像的加权平均

    * 给定项画像 $i$ 和用户画像 $x$，可以使用余弦相似度评估：$u(x,i)=\cos(x,i)=\frac{x\cdot i}{||x||\cdot||i||}$

  * **基于内容的推荐系统的优点：**

    * 不需要其他用户的数据，没有冷启动和稀疏性问题
    * 能够推荐给口味独特的用户
    * 能够推荐新的和不受欢迎的项目，没有 First-Rater 问题
    * 有很好的可解释性：它可以通过列出导致项目被推荐的内容-功能来提供推荐项目的解释

  * **基于内容的推荐系统的缺点**：

    * 难以找到合适的特征向量
    * 难以向新用户推荐项，因为新用户没有用户画像
    * 过度专门化：永远不会推荐用户兴趣之外的项，同时难以处理人多种信兴趣爱好的问题；无法利用其它用户进行推荐质量的评估

* **基于协同滤波的推荐算法** 

  * **基于用户的协同滤波算法**：找出评分与用户 $x$ 评分相似的其他用户的集合，根据集合中用户的评分估计用户 $x$ 对项的评分。

    * 第一步：读取用户-项的**评分矩阵** $R$。

    * 第二步：跟据评分矩阵计算**用户相似度矩阵** $S_U$，在计算相似度时我们选择**皮尔森相关系数**。我们可以将计算出的评分矩阵保存在文件中，以免下次重复计算。

    * 第三步：假定我们要预测用户 $u$ 给项 $i$ 的评分。首先找到于目标用户最相似的 K 个用户 $U_{sim}$，并且这些用户对项 $i$ 有评分记录，根据以下公式计算预测评分：
      $$
      r_{u,i} = \frac{\sum_{v \in U_{sim}} s_{u,v}r_{v,i}}{\sum_{v \in U_{sim}} s_{u,v}}
      $$
      其中，$r_{u,i}$ 指用户 $u$ 对项 $i$ 的预测评分，$s_{u,v}$ 指用户 $u$ 和用户 $v$ 的相似度。

  * **基于项的协同滤波算法**

    * 第一步：读取用户-项的评分矩阵 $R$。

    * 第二步：跟据评分矩阵计算用户相似度矩阵 $S_I$，在计算相似度时我们选择皮尔森相关系数。我们可以将计算出的评分矩阵保存在文件中，以免下次重复计算。

    * 第三步：假定我们要预测用户 $u$ 给项 $i$ 的评分。首先找到于目标项最相似的 K 个项 $I_{sim}$，并且用户 $u$ 对这些项有评分记录，根据以下公式计算预测评分：
      $$
      r_{u,i} = \frac{\sum_{j \in I_{sim}} s_{i,j}r_{v,i}}{\sum_{j \in I_{sim}} s_{i,j}}
      $$
      其中，$r_{u,i}$ 指用户 $u$ 对项 $i$ 的预测评分，$s_{i,j}$ 指项 $i$ 和项 $j$ 的相似度。

  * 计算相关性的几种方法：

    * Jaccard：两个集合A和B交集元素的个数在A、B并集中所占的比例

    * Cosine：$\cos(i,j)=\frac{i\cdot j}{||i||\cdot||j||}$

    * Pearson：
      $$
      sim(x,y) = \frac{\sum_{s\in S_{xy}}({r_{xs}-\overline r_x})({r_{ys}-\overline r_y})}{\sqrt{\sum_{s\in S_{xy}(r_{xs}-\overline r_x)^2}}\sqrt{\sum_{s\in S_{xy}(r_{ys}-\overline r_y)^2}}}
      $$

  * **协同滤波算法的评价**

    * 适用场景：
      * 基于用户的协同滤波算法：具备更强的社交特性，适用于用户少物品多，时效性较强的场景。比如新闻、博客、微内容推荐场景。此外基于用户的协同滤波算法能够为用户发现新的兴趣爱好。
      * 基于项的协同滤波算法：更适用于兴趣变化较为稳定的应用，更接近于个性化的推荐，适合物品少用户多，用户兴趣固定持久，物品更新速度不是太快的场合，比如电影推荐。
    * **协同滤波算法的优点**：适用于任何类型的项，不需要特征选择
    * 协同滤波算法的缺点：
      * **冷启动问题：对于基于用户的协同滤波算法，需要积累足够多的用户，并且用户有一定评分时才能找到一个用户的相似用户，而基于项的协同滤波算法没有此问题。**
      * **稀疏性问题：项的数目一般很多，一个用户对项的评分往往不会很多，评分矩阵是稀疏的，难以找到对相同的项评分过的用户。**
      * First-Rater 问题
      * 新的项、评分较少的项因为评分较少，难以被推荐。

    > - 冷启动问题：
    >   - 用户冷启动：如何给新用户做个性化推荐
    >   - 物品冷启动：如何将新的物品推荐给可能对它感兴趣的用户
    >   - 系统冷启动：如何在新开发的网站(无用户，用户行为，只有部分物品信息)上设计个性化推荐系统，从而使得网站刚发布时就让用户体会到**个性化推荐**。
    > - 冷启动的解决方案：
    >   1. **提供非个性化推荐**，比如说热门排行榜，等用户数据收集到一定的时候，切换为个性化推荐
    >   2. **利用用户注册信息**，**人口统计学信息**；**用户兴趣描述**；**从其它网站导入的用户站外行为**等。
    >   3. **选择合适的物品启动用户的兴趣**，用户登录时对一些物品进行反馈，收集用户对这些物品的兴趣信息，然后给用户推荐和这些物品相似的物品。
    >   4. **利用物品的内容信息**，userCF算法需要解决第一推动力的问题，第一个用户从哪里发现新物品。考虑**利用物品的内容信息**，将**新物品先投放给曾经喜欢过和它内容相似的其他物品的用户**。对于itemCF，只能利用物品的内容信息计算物品的相关程度。基本思路就是将物品转换为关键词向量，通过计算向量之间的相似度(如余弦相似度)，得到物品的相关程度。
    >   5. **采用专家标注**，针对很多系统在建立的时候，既没有用户的行为数据，也没有充足的物品内容信息来计算物品相似度，这时就需要利用专家标注。
    >   6. **利用用户在其他地方已经沉淀的数据进行冷启动**，比如引导用户通过社交网络账号登录，一方面降低注册成本提高转化率，另一方面获取用户的社交网络信息，解决冷启动问题。
    >   7. **利用用户的手机等兴趣偏好进行冷启动**：Android手机开放的比较高，所以在安装自己的app时，就可以顺路了解下手机上还安装了什么其他的app。然后可以总结用户的特点和类型。

  * 评估指标

    * RMSE：（测试样本的）均方误差







































